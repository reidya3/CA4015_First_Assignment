#!/usr/bin/env python
# coding: utf-8

# # Clustering Analysis
# The purpose of our cluster analysis is to:
# - Measure clustering & central tendency.
# - Perform k-means
# - Evaluate the clusters, **particularly**:
#     - unhealthy vs healthy
#     - study vs study

# In[1]:


from pyclustertend import ivat, vat
from sklearn.preprocessing import scale
import pandas as pd
from sklearn.neighbors import NearestNeighbors
from random import sample
from numpy.random import uniform
import numpy as np
from math import isnan
from pyclustertend import ivat
from sklearn.cluster import KMeans
from sklearn.metrics import silhouette_score
import matplotlib.pyplot as plt
from yellowbrick.cluster import SilhouetteVisualizer
from yellowbrick.cluster import SilhouetteVisualizer


# 
# ## Measure Cluster Tendency
# Clustering algorithms such as k-means are used to determine the structure of multi-dimensional data. Clusters are natural disjoint groups. However, K-means will find clusters in data even if none "actually" exist. Therefore, a fundamentally question to ask before applying any clustering algorithms is : Are clusters present at all?
# Ww will measure the clustering tendency of both the choice and reward datasets before subjecting it to k-means. To do this, we employ two methods:
# - Hopkin's stastisic  of randomness
# - VAT (visual assessment of tendency)
# 
# ### Hopkins statistics
# Hopkins statistics {cite:p}`banerjee2004validating` tests the spatial randomness of a dataset i.e. it measures the probability that a given dataset is generated by a uniform distribution. It is based on the difference between the distance from a real point to its nearest neighbor, U, and the distance from a uniformly generated point within the data space to the nearest real data point, W.
# 
# - $H_{0}$: The dataset **is** uniformly distributed 
# - $H_{1}$: The dataset **is not** uniformly distributed 
# 
# $$
# H = \frac{\sum_{i=1}^{m} u_{i}^{d}}{\sum_{i=1}^{m} u_{i}^{d} + \sum_{i=1}^{m} w_{i}^{d}}
# $$
# 
# If the value of the hopkins statistic(H) is close to 1 (above 0.5), we reject $H_{0}$ and can conclude that the dataset is considered significantly clusterable.  Otherwise, we fail to reject $H_{0}$ and can conclude that the dataset is considered significantly uniformly distributed.

# In[2]:


def hopkins(X):
    d = X.shape[1]
    #d = len(vars) # columns
    n = len(X) # rows
    m = int(0.1 * n) # heuristic from article [1]
    nbrs = NearestNeighbors(n_neighbors=1).fit(X.values)
 
    rand_X = sample(range(0, n, 1), m)
 
    ujd = []
    wjd = []
    for j in range(0, m):
        u_dist, _ = nbrs.kneighbors(uniform(np.amin(X,axis=0),np.amax(X,axis=0),d).reshape(1, -1), 2, return_distance=True)
        ujd.append(u_dist[0][1])
        w_dist, _ = nbrs.kneighbors(X.iloc[rand_X[j]].values.reshape(1, -1), 2, return_distance=True)
        wjd.append(w_dist[0][1])
 
    H = sum(ujd) / (sum(ujd) + sum(wjd))
    if isnan(H):
        print(ujd, wjd)
        H = 0
 
    return H


# In[3]:


dim_reduced_choice = pd.read_csv('data/dim_reduced_choice.tsv', sep="\t")
dim_reduced_rewards = pd.read_csv('data/dim_reduced_rewards.tsv', sep="\t")


# For both datasets, we reject $H_{0}$ and can conclude that both datasets have a significant tendency to cluster.

# In[4]:


print(f"Choice's hopkins statistic {hopkins(dim_reduced_choice.iloc[:,2:])}")
print(f"Reward's hopkins statistic {hopkins(dim_reduced_rewards.iloc[:,2:])}") 


# #### IVAT (visual assessment of tendency)
# VAT is visual method of assessing the clustering likelihood of a dataset. VAT creates a minimum spanning tree of observations, where the pairwise distance between those observations is displayed as the black squares of an ordered dissimilarity square-shaped Map. The densely black squares on the diagonal can be understood as the number of clusters. The different shade of black provides insight on not only the numbers of clusters but, also the cluster hierarchy. **Note**, This algorithm is not a substitute for cluster evaluation metrics (Elbow, Silhouette coefficient). It merely suggests if clusters exist in the datasets, so as to avoid conducting cluster analysis on datasets in the first place. IVAT is just an improved version improved version of the VAT algorithm which produce more precise images but is more computational expensive.
# 
# 
# **IVAT MAP of Choices**

# In[5]:


ivat(dim_reduced_choice.iloc[:,2:].values)


# **IVAT MAP of Rewards**
# 

# In[6]:


ivat(dim_reduced_rewards.iloc[:,2:].values)


# From the result of implementing IVAT, it is observed from the maps produced are inconclusive for both datasets. However, as this algorithm is just meant to help us decide if we should go ahead with the cluster analysis or not, we will go ahead with the K-means cluster analysis as both hopkins stastic results were significant.

# ## K-Means
# 
# K-means is a common clustering algorithm. Although a simple clustering algorithm , it has wide application areas including in customer segmentation and Image compression. K-means is a centroid based algorithm that aims to minimize the sum of distances between the points and their respective cluster centroid..  The main steps of this algorithm are:
# 
# - **Step 1**: Choose the number (k) of clusters
# - **Step 2**: Select k random points which will become the inital centroids
# - **Step 3**: Assign all data points to the nearest centroid.
# - **step 4**: Compute the centroid of the newly formed clusters by by taking the
# mean of data instances currently associated with that cluster.
# - **Step 5**: Repeat steps 3 and 4 untill either:
#     - Centroids of newly formed clusters do not change
#     - Points remain in the same cluster
#     - Maximum number of iterations are reached
# 
# But how do we find the optimal number of clusters?
# - Elbow method
# - Silhouette coefficient
# 
# ### Elbow method
# The Elbow method calculates the error or 'distortion' between the data points ($y_{i}$) and their corresponding centroid ($ŷ_{i}$) of N data points for k clusters where k ⋹ {1...10}. The error metric used is the Sum of Squared Error (SSE):
# $$
# SSE = \sum_{i=1}^{N} {(y_i - ŷ_i)^2}
# $$
# 
# We plot these values in an attempt to find an 'elbow' within the curve.

# In[7]:


Sum_of_squared_distances = []
for k in range(1, 10):
    km_choice = KMeans(n_clusters=k, random_state=42)
    km_choice = km_choice.fit(dim_reduced_choice.iloc[:, 2:])
    Sum_of_squared_distances.append(km_choice.inertia_)
plt.plot(K, Sum_of_squared_distances, 'bx-')
plt.xlabel('No of Clusters')
plt.ylabel('Sum_of_squared_distances')
plt.title('Elbow Method For Optimal k for choices dataset')
plt.show()


# We can see that the optimal number of clusters occur at k=2 or k =3. However, the error is quite high at these points. This is most certainly due to the high number of dimensions, we are feeding the algorithm as Euclidean distance suffers in high dimensions. We repeat this process for rewards.  

# In[95]:


Sum_of_squared_distances = []
for k in range(1, 10):
    km_rewards = KMeans(n_clusters=k, random_state=42)
    km_rewards = km_rewards.fit(dim_reduced_rewards.iloc[:, 2:])
    Sum_of_squared_distances.append(km_rewards.inertia_)
plt.plot(K, Sum_of_squared_distances, 'bx-')
plt.xlabel('No of Clusters')
plt.ylabel('Sum_of_squared_distances')
plt.title('Elbow Method For Optimal k for rewards dataset')
plt.show()


# Here, determining the amount of clusters is more ambiguous. But I believe there is a dip at k=4 to k=6 mark.

# ## Sillhoute method

# This method is another method of finding the correct number of clusters(k). Silhouette coefficient for a particular data point ($i$), is defined as:
# $$
# s_{i} = \frac{b_{i} - a_{i}}{max(b_{i}, a_{i})}
# $$
# where:
# - $s_{i}$: the silhouette coefficient, ranging from -1 to 1. A score of 1 (the best) means that data point $i$ is compact in its cluster and far away from other clusters. Conversely, the worst value is -1, while values near 0 denote overlapping clusters.
# - $b_{i}$: average distance between $i$ and all the other data points in its cluster. 
# - $a_{i}$: minimum average distance from $i$ to all clusters to which $i$ does not belong to

# In[109]:


from yellowbrick.cluster import SilhouetteVisualizer
for k in  range(2,6):
    '''
    Create KMeans instance for different number of clusters
    '''
    km = KMeans(n_clusters=k, random_state=42)
    visualizer = SilhouetteVisualizer(km, colors='yellowbrick')
    visualizer.fit(dim_reduced_choice.iloc[:, 2:])
    visualizer.show()


# This lies up with our previous assumptions on the optimal number of clusters (2,3) of the choice dataset. K=4 and 5 seem to be sub optimal due to wide foundations in size of the silhouette plots and as some clusters are below the average silhouette score. Conversely, k=2 and 3 are good picks as 

# In[119]:


from yellowbrick.cluster import SilhouetteVisualizer

for k in  range(4,8):
    '''
    Create KMeans instance for different number of clusters
    '''
    km = KMeans(n_clusters=k, random_state=42)
    visualizer = SilhouetteVisualizer(km, colors='yellowbrick')
    visualizer.fit(dim_reduced_rewards.iloc[:, 2:])
    visualizer.show()


# ## Findings
# 
# ### Healthy vs Unhealthy

# #### (k=2, choice dataset) 

# In[112]:


fig, axs = plt.subplots(2, 2)


# In[ ]:




