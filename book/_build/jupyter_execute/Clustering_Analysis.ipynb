{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clustering Analysis\r\n",
    "The purpose of our cluster analysis is to:\r\n",
    "<ol type = \"a\">\r\n",
    "    <li>Measure clustering & central tendency.</li>\r\n",
    "    <li>Validate assumptions of the k-means algorithm</li>\r\n",
    "    <li>Perform k-means</li>\r\n",
    "    <li>Evaluate the clusters, <b>particularly</b> how unhealthy vs healthy individuals cluster</li>\r\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyclustertend import ivat\r\n",
    "import pandas as pd\r\n",
    "from sklearn.neighbors import NearestNeighbors\r\n",
    "from random import sample\r\n",
    "from numpy.random import uniform\r\n",
    "import numpy as np\r\n",
    "from math import isnan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\r\n",
    "## Measure Cluster Tendency\r\n",
    "Clustering algorithms such as k-means are used to determine the structure of multi-dimensional data. Clusters are natural disjoint groups. However, K-means will find clusters in data even if none \"actually\" exist. Therefore, a fundamentally question to ask before applying any clustering algorithms is : Are clusters present at all?\r\n",
    "Ww will measure the clustering tendency of both the choice and reward datasets before subjecting it to k-means. To do this, we employ two methods:\r\n",
    "- Hopkin's stastisic  of randomness\r\n",
    "- VAT (visual assessment of tendency)\r\n",
    "\r\n",
    "### Hopkins statistics\r\n",
    "Hopkins statistics {cite:p}`banerjee2004validating` tests the spatial randomness of a dataset i.e. it measures the probability that a given dataset is generated by a uniform distribution. It is based on the difference between the distance from a real point to its nearest neighbor, U, and the distance from a uniformly generated point within the data space to the nearest real data point, W.\r\n",
    "\r\n",
    "- $H_{0}$: The dataset **is** uniformly distributed \r\n",
    "- $H_{1}$ The dataset **is not** uniformly distributed \r\n",
    "\r\n",
    "$$\r\n",
    "H = \\frac{\\sum_{i=1}^{m} u_{i}^{d}}{\\sum_{i=1}^{m} u_{i}^{d} + \\sum_{i=1}^{m} w_{i}^{d}}\r\n",
    "$$\r\n",
    "\r\n",
    "If the value of the hopkins statistic(H) is close to 1 (above 0.5), we reject $H_{0}$ and can conclude that the dataset is considered significantly clusterable.  Otherwise, we fail to reject $H_{0}$ and can conclude that the dataset is considered significantly uniformly distributed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hopkins(X):\r\n",
    "    d = X.shape[1]\r\n",
    "    #d = len(vars) # columns\r\n",
    "    n = len(X) # rows\r\n",
    "    m = int(0.1 * n) # heuristic from article [1]\r\n",
    "    nbrs = NearestNeighbors(n_neighbors=1).fit(X.values)\r\n",
    " \r\n",
    "    rand_X = sample(range(0, n, 1), m)\r\n",
    " \r\n",
    "    ujd = []\r\n",
    "    wjd = []\r\n",
    "    for j in range(0, m):\r\n",
    "        u_dist, _ = nbrs.kneighbors(uniform(np.amin(X,axis=0),np.amax(X,axis=0),d).reshape(1, -1), 2, return_distance=True)\r\n",
    "        ujd.append(u_dist[0][1])\r\n",
    "        w_dist, _ = nbrs.kneighbors(X.iloc[rand_X[j]].values.reshape(1, -1), 2, return_distance=True)\r\n",
    "        wjd.append(w_dist[0][1])\r\n",
    " \r\n",
    "    H = sum(ujd) / (sum(ujd) + sum(wjd))\r\n",
    "    if isnan(H):\r\n",
    "        print(ujd, wjd)\r\n",
    "        H = 0\r\n",
    " \r\n",
    "    return H"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dim_reduced_choice = pd.read_csv('data/dim_reduced_choice.tsv', sep=\"\\t\")\r\n",
    "dim_reduced_rewards = pd.read_csv('data/dim_reduced_rewards.tsv', sep=\"\\t\")\r\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For both datasets, we reject $H_{0}$ and can conclude that both datasets have a significant tendency to cluster "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Choice's hopkins statistic 0.6590701860667252\n",
      "Reward's hopkins statistic 0.7422082385277213\n"
     ]
    }
   ],
   "source": [
    "print(f\"Choice's hopkins statistic {hopkins(dim_reduced_choice.iloc[:,2:])}\")\r\n",
    "print(f\"Reward's hopkins statistic {hopkins(dim_reduced_rewards.iloc[:,2:])}\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### VAT (visual assessment of tendency)\r\n",
    "Vat is visual method of assessing the clustering likelihood of a dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'plt' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_39656/812981093.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlogspace\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m100\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mii\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mii\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mN\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mT\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m \u001b[0mcmap\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcm\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcoolwarm\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m \u001b[0mrcParams\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'axes.prop_cycle'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcycler\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcolor\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcmap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlinspace\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mN\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'plt' is not defined"
     ]
    }
   ],
   "source": [
    "# Fixing random state for reproducibility\r\n",
    "np.random.seed(19680801)\r\n",
    "\r\n",
    "N = 10\r\n",
    "data = [np.logspace(0, 1, 100) + np.random.randn(100) + ii for ii in range(N)]\r\n",
    "data = np.array(data).T\r\n",
    "cmap = plt.cm.coolwarm\r\n",
    "rcParams['axes.prop_cycle'] = cycler(color=cmap(np.linspace(0, 1, N)))\r\n",
    "\r\n",
    "\r\n",
    "from matplotlib.lines import Line2D\r\n",
    "custom_lines = [Line2D([0], [0], color=cmap(0.), lw=4),\r\n",
    "                Line2D([0], [0], color=cmap(.5), lw=4),\r\n",
    "                Line2D([0], [0], color=cmap(1.), lw=4)]\r\n",
    "\r\n",
    "fig, ax = plt.subplots(figsize=(10, 5))\r\n",
    "lines = ax.plot(data)\r\n",
    "ax.legend(custom_lines, ['Cold', 'Medium', 'Hot']);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is a lot more that you can do with outputs (such as including interactive outputs)\n",
    "with your book. For more information about this, see [the Jupyter Book documentation](https://jupyterbook.org)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "00a0075937c46e4fa2684306921a6efe0826486343b024167c91da4e96a7c8cc"
  },
  "kernelspec": {
   "display_name": "Python 3.8.3 64-bit ('venv': venv)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}