{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Federated K-Means\r\n",
    "In this section, we attempt to form a federated k-means algorithm to preserve the privacy of the individual labs. \r\n",
    "\r\n",
    "**Federated learning:**  First introduced by Google, federated learning trains an algorithm across multiple decentralized servers/devices with local data samples. There is **no** data sharing between servers. This is in contrast to the standard way of training a machine learning algorithm where all of the training dataset is uploaded to one server. This technique adrress critical issues relating to data privacy and security \r\n",
    "\r\n",
    "## Child (Study):\r\n",
    "### 1. Base k-means:\r\n",
    "The original K-Means algorithm is only trained/ on a given dataset once. There is no update method. In this approach, we split a study's datastet 75:25.  75% of a study's data is trained using normal K-means. It is important to note that all child devices are independent of one another. \r\n",
    "### 2. Update:\r\n",
    "In an attempt to resemble real-world federated learning where a new data point is generated on a device, we add a update step to base K-means. Rather then recompute k-means, We iterate through the remaining 25% of data and perform the following steps \r\n",
    "1. Convert new data point to numpy array\r\n",
    "2. Find the minimum Euclidean distance between that new point $X_{i}$  and the cluster centers (T) to find the closest cluster centre ($C_{i}$).\r\n",
    "$$\r\n",
    "Minimum Distance = min((X_{i} - C_{1})^2.............(X_{i} - C_{T})^2)\r\n",
    "$$\r\n",
    "3. Transform the cluster centre $C_{i}$ by doing the operation below. N equals the number of participants assigned to that cluster thus far i.e. before the new data point :\r\n",
    "$$\r\n",
    "TransformedClusterCentre = \\frac{((C_{i} * N) + X_{i})}{N+1}\r\n",
    "$$\r\n",
    "3. Then, the new data point is added to the cluster, and the transformed cluster centre is the  new cluster centre.\r\n",
    "## Parent (Server):\r\n",
    "### 3. Aggregate & Compute weighted average:\r\n",
    "Once all of the child devices (S) has completed their update phase, their cluster centres are added to the parent server. Then, we compute another K-means run to find the optimal number of k centroids."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'sklearn_extra'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_3828/3986984131.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mscipy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcluster\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvq\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mkmeans2\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn_extra\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcluster\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mKMedoids\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'sklearn_extra'"
     ]
    }
   ],
   "source": [
    "from sklearn.cluster import KMeans\r\n",
    "import pandas as pd\r\n",
    "import numpy as np\r\n",
    "from scipy.cluster.vq import kmeans2\r\n",
    "from sklearn_extra.cluster import KMedoids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "dim_reduced_2d = pd.read_csv('data/dim_reduced_2d.tsv', sep=\"\\t\")\r\n",
    "dim_reduced_3d = pd.read_csv('data/dim_reduced_3d.tsv', sep=\"\\t\")\r\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we are just creating variables to hold the data from each study. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "studies_list = ['Horstmann', 'Kjome', 'Maia', 'SteingroverInPrep', 'Premkumar','Wood', 'Worthy', 'Ahn']\r\n",
    "for study in studies_list:\r\n",
    "    globals()[f'{study}_study'] = dim_reduced_2d[dim_reduced_2d['study'] == study]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Child K-means class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "class child_kmeans(KMeans):\r\n",
    "    \"\"\"\r\n",
    "    A python class that executes the original k-means algorithm on 75% of the available data \r\n",
    "        from a study. Leftover data is used to update the cluster centres\r\n",
    "        as described above. Inherits from scikit-learn's K-means class\r\n",
    "    \"\"\"\r\n",
    "    \r\n",
    "    def __init__(self,\r\n",
    "                df,\r\n",
    "                n_clusters):\r\n",
    "        super().__init__(n_clusters=n_clusters, random_state=42)\r\n",
    "        self.df = df\r\n",
    "        self.update_df, self.base_df = np.split(df, [int(.25*len(df))])\r\n",
    "        # map cluster index to number of particpants e.g. <0:77> \r\n",
    "        self.cluster_index_num = dict()\r\n",
    "        # map cluster index to number of particpants e.g. <0:array([-0.96292967,  1.03276864])> \r\n",
    "        # Necessary as numpy array is unhashable \r\n",
    "        self.index_cluster_centre = dict()\r\n",
    "\r\n",
    "    \r\n",
    "    def find_closest_cluster(self, new_data_point):\r\n",
    "        min_dist =  float(\"inf\")\r\n",
    "        min_cluster_centre= None\r\n",
    "        cluster_index = None\r\n",
    "        for current_key, cluster in self.index_cluster_centre.items():\r\n",
    "            current_dist = (np.sum(np.square(new_data_point - cluster)))\r\n",
    "            if current_dist < min_dist:\r\n",
    "                min_cluster_centre = cluster\r\n",
    "                min_dist = current_dist\r\n",
    "                cluster_index = current_key\r\n",
    "        return cluster_index, min_cluster_centre\r\n",
    "\r\n",
    "    def update(self):\r\n",
    "        for index, row in self.update_df.iterrows():\r\n",
    "            new_data_point = np.array(row)\r\n",
    "            cluster_index, closest_cluster = self.find_closest_cluster(new_data_point)\r\n",
    "            num_subjects = self.cluster_index_num[cluster_index]\r\n",
    "            self.index_cluster_centre[cluster_index] = np.divide(((closest_cluster * num_subjects) + new_data_point), num_subjects+1)\r\n",
    "            self.cluster_index_num[cluster_index] += 1\r\n",
    "    \r\n",
    "\r\n",
    "    def create_maps(self):\r\n",
    "        cluster_indexes, counts = np.unique(self.labels_, return_counts=True)\r\n",
    "        self.cluster_index_num = {cluster_index:count for cluster_index, count in zip(cluster_indexes, counts) }\r\n",
    "        self.index_cluster_centre = {cluster_index: self.cluster_centers_[cluster_index] for cluster_index in cluster_indexes}\r\n",
    "\r\n",
    "    def run(self):\r\n",
    "        super().fit(self.base_df)\r\n",
    "        self.create_maps()\r\n",
    "        self.update() \r\n",
    "        updated_cluster_centres = [np.array(cluster_centre) for cluster_centre in self.index_cluster_centre.values()] \r\n",
    "        return updated_cluster_centres, self.df.shape[0]    \r\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parent K-means class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "class parent_kmeans(kmeans):\r\n",
    "    \"\"\"\r\n",
    "    A python class that retrieves cluster centres from\r\n",
    "        each study, and then computes another k-means algorithim \r\n",
    "    \"\"\"\r\n",
    "\r\n",
    "    def __init__(self) -> None:\r\n",
    "        self.n_participants = []\r\n",
    "        self.cluster_centres = []\r\n",
    "        self.federated_cluster_centres = None\r\n",
    "\r\n",
    "    def add(self, cluster_centre, n_participant):\r\n",
    "        self.cluster_centres.extend(cluster_centre)\r\n",
    "        self.n_participants.append(n_participant)\r\n",
    "\r\n",
    "    def weighted_average(self):\r\n",
    "        child_weighted_avg = np.average(self.cluster_centres, weights=self.n_participants, axis=0)        \r\n",
    "        return child_weighted_avg\r\n",
    "\r\n",
    "    def update_cluster_centre(self):\r\n",
    "        self.federated_cluster_centres = self.weighted_average()\r\n",
    "        self.n_participants = []\r\n",
    "        self.cluster_centres = []\r\n",
    "    \r\n",
    "    def get_new_centres(self):\r\n",
    "        return self.federated_cluster_centres\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "parent_server = parent_kmeans()\r\n",
    "\r\n",
    "for study in studies_list:\r\n",
    "    # First retrieving the cluster centres and number of particpants  from a study\r\n",
    "    study_cluster_centres, n_particpants = child_kmeans(globals()[f'{study}_study'].iloc[:,2:], n_clusters=3).run()\r\n",
    "    list_cluster_cenntres.extend(study_cluster_centres)\r\n",
    "    # Adding that information to the parent server \r\n",
    "    parent_server.add(cluster_centre=study_cluster_centres, n_participant=n_particpants)\r\n",
    "\r\n",
    "# Calculating the new federated cluster centres\r\n",
    "parent_server.update_cluster_centre()\r\n",
    "# Retreving the cluster centres from Federated K-means and normal K-means    \r\n",
    "fkm_cluster_centres = KMedoids(n_clusters=3,random_state=42).fit(list_cluster_cenntres).cluster_centers_\r\n",
    "km_clusters_centres = KMeans(n_clusters=3,random_state=42).fit(dim_reduced_2d.iloc[:,2:]).cluster_centers_\r\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before evaluating the results, an important consideration should be noted. As with any K-Means algorithm, results may vary with each run or input seed of the algorithm as the algorithm's performance is heavily dependent on the initial clusters chosen. We evaluate our algorithm on the 2d dataset with k=3 and seed=42. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "\r\n",
    "def calulate_SSE(df, fkm_cluster_centres, km_clusters_centres):\r\n",
    "    \"\"\"\r\n",
    "    Calculates k-mean's objective function (Sum Square Error) for both federared K-means\r\n",
    "        algorithm and the original K-means algorithm \r\n",
    "    \r\n",
    "    :param df: Dataframe containing data from the many labs paper\r\n",
    "    :param fkm_cluster_centres: Cluster centres of the Federated K-means Algo.\r\n",
    "    :param km_clusters_centres: Cluster centres of the original K-means Algo.\r\n",
    "    \"\"\"\r\n",
    "    df[\"fkm_SSE\"] = None\r\n",
    "    df[\"km_SSE\"] = None\r\n",
    "    for index, subject in df.iterrows():\r\n",
    "        subject_dim = np.array(subject[[\"component_1\",\"component_2\"]])\r\n",
    "        df.iloc[index, -2] = min([(np.sum(np.square(subject_dim - cluster))) for cluster in fkm_cluster_centres])\r\n",
    "        df.iloc[index, -1] =  min([np.sum(np.square(subject_dim - cluster)) for cluster in km_clusters_centres])\r\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_2d_df = calulate_SSE(dim_reduced_2d, fkm_cluster_centres, km_clusters_centres)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Federated K-mean SSE: 1498.2800508864157\n",
      "K-mean SSE: 1362.19601035772\n"
     ]
    }
   ],
   "source": [
    "print(f'Federated K-mean SSE: {evaluate_2d_df[\"fkm_SSE\"].sum()}')\r\n",
    "print(f'K-mean SSE: {evaluate_2d_df[\"km_SSE\"].sum()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.00246288e-03 -3.38455730e-01]\n",
      " [-2.51137329e+00  5.80883100e-01]\n",
      " [ 2.75575866e+00 -4.04762233e-02]]\n"
     ]
    }
   ],
   "source": [
    "print(fkm_cluster_centres)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our chosen approach only results in a approx 26% increase in SSE when compared to the original centralized K-means algorithm"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "00a0075937c46e4fa2684306921a6efe0826486343b024167c91da4e96a7c8cc"
  },
  "kernelspec": {
   "display_name": "Python 3.8.3 64-bit ('venv': venv)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}